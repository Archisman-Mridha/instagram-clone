apiVersion: debezium.io/v1alpha1
kind: DebeziumServer 
metadata:
  name: users-table
  namespace: debezium

spec:
  image: quay.io/debezium/server:2.4.0.Final

  quarkus: 
    config:
      log.console.json: false
      kubernetes-config.enabled: true
      kubernetes-config.secrets: postgres-credentials

  sink:
    type: kafka
    config:
      producer.bootstrap.servers: main-kafka-bootstrap.strimzi:9092
      producer.key.serializer: org.apache.kafka.common.serialization.StringSerializer
      producer.value.serializer: org.apache.kafka.common.serialization.StringSerializer

  source:
    class: io.debezium.connector.postgresql.PostgresConnector
    config:
      offset.storage.file.filename: /debezium/data/offsets.dat

      # connector.id: users-table-source-connector
      # connector.displayName: users-table-source-connector

      database.server.name: users
      database.hostname: main-rw.cloudnative-pg
      database.port: 5432
      database.dbname: instagram_clone
      database.user: ${POSTGRES_USER}
      database.password: ${POSTGRES_PASSWORD}
      database.history: io.debezium.relational.history.FileDatabaseHistory

      # Controls whether a delete event is followed by a tombstone event.
      # |
      # |- true - a delete operation is represented by a delete event and a subsequent tombstone event.
      # |
      # |- false - only a delete event is emitted.
      #
      # After a source record is deleted, emitting a tombstone event (the default behavior) allows
      # Kafka to completely delete all events that pertain to the key of the deleted row in case log
      # compaction is enabled for the topic.
      tombstones.on.delete: "false"

      # Specifies whether to skip publishing messages when there is no change in included columns.
      # This would essentially filter messages if there is no change in columns included as per
      # column.include.list or column.exclude.list properties.
      # Note: Only works when REPLICA IDENTITY of the table is set to FULL
      skip.messages.without.change: "true"

      # The name of the PostgreSQL logical decoding slot that was created for streaming changes from
      # a particular plug-in for a particular database/schema. The server uses this slot to stream
      # events to the Debezium connector that you are configuring.
      slot.name: users_table_source_connector

      schema.include.list: public
      table.include.list: public.users
      column.include.list: public.users.id,public.users.name,public.users.username

      # The name of the PostgreSQL logical decoding plug-in installed on the PostgreSQL server.
      # Supported values are decoderbufs, and pgoutput.
      plugin.name: pgoutput

      # A publication can be defined on any physical replication primary. The node where a publication
      # is defined is referred to as publisher. A publication is a set of changes generated from a
      # table or a group of tables, and might also be described as a change set or replication set.
      # Each publication exists in only one database.
      # This setting determines how creation of a publication should work.
      # (If a publication exists, the connector uses it. If a publication does not exist, the
      # connector creates a publication for all tables in the database for which the connector is
      # capturing changes. For the connector to create a publication it must access the database
      # through a database user account that has permission to create publications and perform\
      # replications.)
      publication.autocreate.mode: all_tables

      # the Debezium event flattening single message transformation (SMT).
      # transforms: unwrap
      # transforms.unwrap.type: io.debezium.transforms.ExtractNewRecordState
      # transforms.unwrap.add.fields: op,
      # transforms.unwrap.drop.tombstones: false
      # transforms.unwrap.delete.handling.mode: rewrite

      # Topic prefix that provides a namespace for the particular PostgreSQL database server or
      # cluster in which Debezium is capturing changes. The prefix should be unique across all other
      # connectors, since it is used as a topic name prefix for all Kafka topics that receive records
      # from this connector. Only alphanumeric characters, hyphens, dots and underscores must be used
      # in the database server logical name.
      topic.prefix: db-events